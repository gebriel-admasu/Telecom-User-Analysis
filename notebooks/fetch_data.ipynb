{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add scripts path\n",
    "sys.path.append(os.path.abspath('../scripts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import connect_to_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM xdr_data;\"\n",
    "df = connect_to_db(query)\n",
    "if df is not None:\n",
    "    print(\"successfuly connected\")\n",
    "else:\n",
    "    print(\"failed to connect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_handsets = df['Handset Type'].value_counts().head(10)\n",
    "print(\"Top 10 Handsets:\\n\", top_handsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_manufacturers = df['Handset Manufacturer'].value_counts().head(3)\n",
    "print(\"Top 3 Handset Manufacturers:\\n\", top_manufacturers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for manufacturer in top_manufacturers.index:\n",
    "    top_handsets_per_manufacturer = df[df['Handset Manufacturer'] == manufacturer]['Handset Type'].value_counts().head(5)\n",
    "    print(f\"Top 5 Handsets for {manufacturer}:\\n\", top_handsets_per_manufacturer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze User Behavior on Applications Analyze User Behavior on Applications\n",
    "user_behavior = df.groupby('IMSI').agg({\n",
    "    'Bearer Id': 'sum',\n",
    "    'Dur. (ms)': 'sum',\n",
    "    'Total DL (Bytes)': 'sum',\n",
    "    'Total UL (Bytes)': 'sum',\n",
    "})\n",
    "user_behavior['total_data'] = user_behavior['Total DL (Bytes)'] + user_behavior['Total UL (Bytes)']\n",
    "print(user_behavior.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values Summary:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill missing numeric columns with the mean\n",
    "numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "\n",
    "# Fill missing categorical columns with 'Unknown'\n",
    "categorical_cols = df.select_dtypes(exclude=[\"number\"]).columns\n",
    "df[categorical_cols] = df[categorical_cols].fillna(\"Unknown\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to handle outliers using the IQR method\n",
    "def handle_outliers(col):\n",
    "    Q1 = col.quantile(0.25)\n",
    "    Q3 = col.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return np.clip(col, lower_bound, upper_bound)\n",
    "\n",
    "# Apply outlier treatment for numeric columns\n",
    "for col in numeric_cols:\n",
    "    df[col] = handle_outliers(df[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of data types and unique values\n",
    "variable_summary = pd.DataFrame({\n",
    "    \"Data Type\": df.dtypes,\n",
    "    \"Unique Values\": df.nunique()\n",
    "})\n",
    "print(\"Variable Summary:\")\n",
    "print(variable_summary)\n",
    "\n",
    "# Save this summary to a CSV for presentation purposes\n",
    "variable_summary.to_csv(\"variable_summary.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segment users into top five decile classes based on Dur. (ms) and compute total data for each decile.\n",
    "df['total_data'] = df['Total DL (Bytes)'] + df['Total UL (Bytes)']\n",
    "df['duration_decile'] = pd.qcut(df['Dur. (ms)'], 5, labels=False)\n",
    "\n",
    "# Compute total data per decile\n",
    "decile_data = df.groupby('duration_decile')['total_data'].sum().reset_index()\n",
    "print(\"Decile Data Summary:\")\n",
    "print(decile_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_metrics = df.describe()\n",
    "print(\"Basic Metrics Summary:\")\n",
    "print(basic_metrics)\n",
    "\n",
    "# Save to CSV for reporting\n",
    "basic_metrics.to_csv(\"basic_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-Graphical Univariate Analysis\n",
    "\n",
    "dispersion_params = df[numeric_cols].agg(['mean', 'median', 'std', 'var', 'min', 'max'])\n",
    "print(\"Dispersion Parameters:\")\n",
    "print(dispersion_params)\n",
    "\n",
    "# Save to CSV for reporting\n",
    "dispersion_params.to_csv(\"dispersion_params.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for numeric variables\n",
    "df[numeric_cols].hist(bins=30, figsize=(20, 15))\n",
    "plt.suptitle(\"Histograms for Numeric Variables\")\n",
    "plt.savefig(\"histograms.png\")\n",
    "\n",
    "# Boxplot for numeric variables\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=df[numeric_cols])\n",
    "plt.title(\"Boxplot for Numeric Variables\")\n",
    "plt.savefig(\"boxplot.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Scatterplot for total data vs applications (with log transform)\n",
    "app_cols = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',\n",
    "            'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']\n",
    "\n",
    "for col in app_cols:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Log-transform the data (avoid log(0) by adding a small constant)\n",
    "    log_x = np.log1p(df[col])  # log(1 + x) ensures no issues with zero values\n",
    "    log_y = np.log1p(df['total_data'])\n",
    "    \n",
    "    # Scatterplot with log-transformed data\n",
    "    sns.scatterplot(x=log_x, y=log_y)\n",
    "    plt.title(f\"Log-Transformed Total Data vs {col}\")\n",
    "    plt.xlabel(f\"Log {col}\")\n",
    "    plt.ylabel(\"Log Total Data\")\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Apply log transformation\n",
    "df['log_social_media_dl'] = np.log1p(df['Social Media DL (Bytes)'])\n",
    "df['log_total_data'] = np.log1p(df['total_data'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=df['log_social_media_dl'], y=df['log_total_data'], alpha=0.5)\n",
    "plt.title(\"Scatter Plot (Log-Transformed): Total Data vs Social Media DL\")\n",
    "plt.xlabel(\"Log(Social Media DL Bytes)\")\n",
    "plt.ylabel(\"Log(Total Data)\")\n",
    "plt.savefig(\"log_scatter_social_media_dl.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr_matrix = df[app_cols].corr()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(corr_matrix)\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.savefig(\"correlation_heatmap.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction (PCA)\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df[app_cols])\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Variance explained by each component\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot PCA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], s=10, alpha=0.6)\n",
    "plt.title(\"PCA Results\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating engagement metrics per user\n",
    "\n",
    "engagement_metrics = df.groupby('MSISDN/Number').agg({\n",
    "    'Dur. (ms)': 'sum',  \n",
    "    'Total DL (Bytes)': 'sum',   \n",
    "    'Total UL (Bytes)': 'sum',   \n",
    "    'Bearer Id': 'count'      \n",
    "}).rename(columns={\n",
    "    'Dur. (ms)': 'Total_Duration',\n",
    "    'Total DL (Bytes)': 'Total_DL',\n",
    "    'Total UL (Bytes)': 'Total_UL',\n",
    "    'Bearer Id': 'Session_Frequency'\n",
    "})\n",
    "\n",
    "# Total traffic (download + upload)\n",
    "engagement_metrics['Total_Traffic'] = engagement_metrics['Total_DL'] + engagement_metrics['Total_UL']\n",
    "\n",
    "# Top 10 customers per engagement metric\n",
    "top_10_duration = engagement_metrics.nlargest(10, 'Total_Duration')\n",
    "top_10_traffic = engagement_metrics.nlargest(10, 'Total_Traffic')\n",
    "top_10_frequency = engagement_metrics.nlargest(10, 'Session_Frequency')\n",
    "\n",
    "print(\"Top 10 customers by duration:\")\n",
    "print(top_10_duration)\n",
    "\n",
    "print(\"Top 10 customers by traffic:\")\n",
    "print(top_10_traffic)\n",
    "\n",
    "print(\"Top 10 customers by session frequency:\")\n",
    "print(top_10_frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Metrics and Perform K-Means Clustering\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Normalize the metrics\n",
    "scaler = MinMaxScaler()\n",
    "engagement_metrics_normalized = scaler.fit_transform(engagement_metrics[['Total_Duration', 'Total_Traffic', 'Session_Frequency']])\n",
    "\n",
    "# Run k-means (k=3)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "engagement_metrics['Cluster'] = kmeans.fit_predict(engagement_metrics_normalized)\n",
    "\n",
    "print(\"Cluster Assignments:\")\n",
    "print(engagement_metrics['Cluster'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Min, Max, Average, and Total Metrics for Each Cluster\n",
    "\n",
    "\n",
    "cluster_metrics = engagement_metrics.groupby('Cluster').agg({\n",
    "    'Total_Duration': ['min', 'max', 'mean', 'sum'],\n",
    "    'Total_Traffic': ['min', 'max', 'mean', 'sum'],\n",
    "    'Session_Frequency': ['min', 'max', 'mean', 'sum']\n",
    "})\n",
    "\n",
    "print(\"Cluster Metrics:\")\n",
    "print(cluster_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate user total traffic per application\n",
    "app_cols = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',\n",
    "            'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']\n",
    "\n",
    "application_metrics = []  # Store metrics for all applications\n",
    "\n",
    "for app_col in app_cols:\n",
    "    # Calculate total traffic for each user (MSISDN/Number) for this application\n",
    "    app_data = df.groupby('MSISDN/Number').agg({\n",
    "        app_col: 'sum',  # Aggregate the download traffic for this app\n",
    "        'Total UL (Bytes)': 'sum'  # Aggregate the upload traffic\n",
    "    }).rename(columns={\n",
    "        app_col: 'Total_DL',  # Rename download column\n",
    "        'Total UL (Bytes)': 'Total_UL'  # Rename upload column\n",
    "    })\n",
    "    \n",
    "    # Compute total traffic (download + upload)\n",
    "    app_data['Total_Traffic'] = app_data['Total_DL'] + app_data['Total_UL']\n",
    "    \n",
    "    # Add application name as a column\n",
    "    app_data['Application'] = app_col.replace(' DL (Bytes)', '')  # Clean app name\n",
    "    \n",
    "    # Append to application_metrics list\n",
    "    application_metrics.append(app_data)\n",
    "\n",
    "# Concatenate all application metrics into a single DataFrame\n",
    "application_metrics = pd.concat(application_metrics)\n",
    "\n",
    "# Top 10 users per application\n",
    "top_users_per_app = application_metrics.groupby('Application').apply(\n",
    "    lambda x: x.nlargest(10, 'Total_Traffic')\n",
    ")\n",
    "\n",
    "print(\"Top 10 users per application:\")\n",
    "print(top_users_per_app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the top 3 applications by total traffic\n",
    "top_apps = application_metrics.groupby('Application')['Total_Traffic'].sum().nlargest(3).index\n",
    "\n",
    "# Filter data for top 3 applications\n",
    "top_app_data = application_metrics[application_metrics['Application'].isin(top_apps)]\n",
    "\n",
    "# Plot traffic distribution\n",
    "sns.barplot(x='Application', y='Total_Traffic', data=top_app_data, estimator='sum', ci=None)\n",
    "plt.title(\"Top 3 Most Used Applications by Total Traffic\")\n",
    "plt.xlabel(\"Application\")\n",
    "plt.ylabel(\"Total Traffic (Bytes)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method\n",
    "sse = []\n",
    "k_range = range(1, 10)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(engagement_metrics_normalized)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_range, sse, marker='o')\n",
    "plt.title(\"Elbow Method for Optimal K\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Sum of Squared Errors (SSE)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
