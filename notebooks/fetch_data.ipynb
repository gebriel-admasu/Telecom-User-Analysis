{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add scripts path\n",
    "sys.path.append(os.path.abspath('../scripts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import connect_to_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM xdr_data;\"\n",
    "df = connect_to_db(query)\n",
    "if df is not None:\n",
    "    print(\"successfuly connected\")\n",
    "else:\n",
    "    print(\"failed to connect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_handsets = df['Handset Type'].value_counts().head(10)\n",
    "print(\"Top 10 Handsets:\\n\", top_handsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_manufacturers = df['Handset Manufacturer'].value_counts().head(3)\n",
    "print(\"Top 3 Handset Manufacturers:\\n\", top_manufacturers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for manufacturer in top_manufacturers.index:\n",
    "    top_handsets_per_manufacturer = df[df['Handset Manufacturer'] == manufacturer]['Handset Type'].value_counts().head(5)\n",
    "    print(f\"Top 5 Handsets for {manufacturer}:\\n\", top_handsets_per_manufacturer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze User Behavior on Applications Analyze User Behavior on Applications\n",
    "user_behavior = df.groupby('IMSI').agg({\n",
    "    'Bearer Id': 'sum',\n",
    "    'Dur. (ms)': 'sum',\n",
    "    'Total DL (Bytes)': 'sum',\n",
    "    'Total UL (Bytes)': 'sum',\n",
    "})\n",
    "user_behavior['total_data'] = user_behavior['Total DL (Bytes)'] + user_behavior['Total UL (Bytes)']\n",
    "print(user_behavior.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values Summary:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill missing numeric columns with the mean\n",
    "numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "\n",
    "# Fill missing categorical columns with 'Unknown'\n",
    "categorical_cols = df.select_dtypes(exclude=[\"number\"]).columns\n",
    "df[categorical_cols] = df[categorical_cols].fillna(\"Unknown\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to handle outliers using the IQR method\n",
    "def handle_outliers(col):\n",
    "    Q1 = col.quantile(0.25)\n",
    "    Q3 = col.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return np.clip(col, lower_bound, upper_bound)\n",
    "\n",
    "# Apply outlier treatment for numeric columns\n",
    "for col in numeric_cols:\n",
    "    df[col] = handle_outliers(df[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of data types and unique values\n",
    "variable_summary = pd.DataFrame({\n",
    "    \"Data Type\": df.dtypes,\n",
    "    \"Unique Values\": df.nunique()\n",
    "})\n",
    "print(\"Variable Summary:\")\n",
    "print(variable_summary)\n",
    "\n",
    "# Save this summary to a CSV for presentation purposes\n",
    "variable_summary.to_csv(\"variable_summary.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segment users into top five decile classes based on Dur. (ms) and compute total data for each decile.\n",
    "df['total_data'] = df['Total DL (Bytes)'] + df['Total UL (Bytes)']\n",
    "df['duration_decile'] = pd.qcut(df['Dur. (ms)'], 5, labels=False)\n",
    "\n",
    "# Compute total data per decile\n",
    "decile_data = df.groupby('duration_decile')['total_data'].sum().reset_index()\n",
    "print(\"Decile Data Summary:\")\n",
    "print(decile_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_metrics = df.describe()\n",
    "print(\"Basic Metrics Summary:\")\n",
    "print(basic_metrics)\n",
    "\n",
    "# Save to CSV for reporting\n",
    "basic_metrics.to_csv(\"basic_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-Graphical Univariate Analysis\n",
    "\n",
    "dispersion_params = df[numeric_cols].agg(['mean', 'median', 'std', 'var', 'min', 'max'])\n",
    "print(\"Dispersion Parameters:\")\n",
    "print(dispersion_params)\n",
    "\n",
    "# Save to CSV for reporting\n",
    "dispersion_params.to_csv(\"dispersion_params.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for numeric variables\n",
    "df[numeric_cols].hist(bins=30, figsize=(20, 15))\n",
    "plt.suptitle(\"Histograms for Numeric Variables\")\n",
    "plt.savefig(\"histograms.png\")\n",
    "\n",
    "# Boxplot for numeric variables\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=df[numeric_cols])\n",
    "plt.title(\"Boxplot for Numeric Variables\")\n",
    "plt.savefig(\"boxplot.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Scatterplot for total data vs applications (with log transform)\n",
    "app_cols = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',\n",
    "            'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']\n",
    "\n",
    "for col in app_cols:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Log-transform the data (avoid log(0) by adding a small constant)\n",
    "    log_x = np.log1p(df[col])  # log(1 + x) ensures no issues with zero values\n",
    "    log_y = np.log1p(df['total_data'])\n",
    "    \n",
    "    # Scatterplot with log-transformed data\n",
    "    sns.scatterplot(x=log_x, y=log_y)\n",
    "    plt.title(f\"Log-Transformed Total Data vs {col}\")\n",
    "    plt.xlabel(f\"Log {col}\")\n",
    "    plt.ylabel(\"Log Total Data\")\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Apply log transformation\n",
    "df['log_social_media_dl'] = np.log1p(df['Social Media DL (Bytes)'])\n",
    "df['log_total_data'] = np.log1p(df['total_data'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=df['log_social_media_dl'], y=df['log_total_data'], alpha=0.5)\n",
    "plt.title(\"Scatter Plot (Log-Transformed): Total Data vs Social Media DL\")\n",
    "plt.xlabel(\"Log(Social Media DL Bytes)\")\n",
    "plt.ylabel(\"Log(Total Data)\")\n",
    "plt.savefig(\"log_scatter_social_media_dl.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr_matrix = df[app_cols].corr()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(corr_matrix)\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.savefig(\"correlation_heatmap.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction (PCA)\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df[app_cols])\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Variance explained by each component\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot PCA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], s=10, alpha=0.6)\n",
    "plt.title(\"PCA Results\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TASK 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
