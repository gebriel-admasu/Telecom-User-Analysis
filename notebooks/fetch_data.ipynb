{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add scripts path\n",
    "sys.path.append(os.path.abspath('../scripts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import connect_to_db\n",
    "from data_cleaning import  clean_numeric_columns, remove_outliers\n",
    "from data_formatting import format_columns, convert_time_to_hours, format_traffic_data\n",
    "from data_transform import perform_clustering, aggregate_data, top_n_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM xdr_data;\"\n",
    "df = connect_to_db(query)\n",
    "if df is not None:\n",
    "    print(\"successfuly connected\")\n",
    "else:\n",
    "    print(\"failed to connect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_handsets = df['Handset Type'].value_counts().head(10)\n",
    "print(\"Top 10 Handsets:\\n\", top_handsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_manufacturers = df['Handset Manufacturer'].value_counts().head(3)\n",
    "print(\"Top 3 Handset Manufacturers:\\n\", top_manufacturers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for manufacturer in top_manufacturers.index:\n",
    "    top_handsets_per_manufacturer = df[df['Handset Manufacturer'] == manufacturer]['Handset Type'].value_counts().head(5)\n",
    "    print(f\"Top 5 Handsets for {manufacturer}:\\n\", top_handsets_per_manufacturer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze User Behavior on Applications Analyze User Behavior on Applications\n",
    "user_behavior = df.groupby('IMSI').agg({\n",
    "    'Bearer Id': 'sum',\n",
    "    'Dur. (ms)': 'sum',\n",
    "    'Total DL (Bytes)': 'sum',\n",
    "    'Total UL (Bytes)': 'sum',\n",
    "})\n",
    "user_behavior['total_data'] = user_behavior['Total DL (Bytes)'] + user_behavior['Total UL (Bytes)']\n",
    "print(user_behavior.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for numeric columns\n",
    "numeric_columns = df.select_dtypes(include=[\"number\"]).columns\n",
    "df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())\n",
    "\n",
    "# Fill missing values for non-numeric columns\n",
    "non_numeric_columns = df.select_dtypes(exclude=[\"number\"]).columns\n",
    "df[non_numeric_columns] = df[non_numeric_columns].fillna(\"Unknown\")\n",
    "\n",
    "# Verify there are no missing values\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaning import clean_handset_data\n",
    "df = clean_handset_data(df,\"Handset Type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to handle outliers using the IQR method\n",
    "def handle_outliers(col):\n",
    "    Q1 = col.quantile(0.25)\n",
    "    Q3 = col.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return np.clip(col, lower_bound, upper_bound)\n",
    "numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "# Apply outlier treatment for numeric columns\n",
    "for col in numeric_cols:\n",
    "    df[col] = handle_outliers(df[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of data types and unique values\n",
    "variable_summary = pd.DataFrame({\n",
    "    \"Data Type\": df.dtypes,\n",
    "    \"Unique Values\": df.nunique()\n",
    "})\n",
    "print(\"Variable Summary:\")\n",
    "print(variable_summary)\n",
    "\n",
    "# Save this summary to a CSV for presentation purposes\n",
    "variable_summary.to_csv(\"variable_summary.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segment users into top five decile classes based on Dur. (ms) and compute total data for each decile.\n",
    "df['total_data'] = df['Total DL (Bytes)'] + df['Total UL (Bytes)']\n",
    "df['duration_decile'] = pd.qcut(df['Dur. (ms)'], 5, labels=False)\n",
    "\n",
    "# Compute total data per decile\n",
    "decile_data = df.groupby('duration_decile')['total_data'].sum().reset_index()\n",
    "print(\"Decile Data Summary:\")\n",
    "print(decile_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_metrics = df.describe()\n",
    "print(\"Basic Metrics Summary:\")\n",
    "print(basic_metrics)\n",
    "\n",
    "# Save to CSV for reporting\n",
    "basic_metrics.to_csv(\"basic_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-Graphical Univariate Analysis\n",
    "\n",
    "dispersion_params = df[numeric_cols].agg(['mean', 'median', 'std', 'var', 'min', 'max'])\n",
    "print(\"Dispersion Parameters:\")\n",
    "print(dispersion_params)\n",
    "\n",
    "# Save to CSV for reporting\n",
    "dispersion_params.to_csv(\"dispersion_params.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for numeric variables\n",
    "df[numeric_cols].hist(bins=30, figsize=(20, 15))\n",
    "plt.suptitle(\"Histograms for Numeric Variables\")\n",
    "plt.savefig(\"histograms.png\")\n",
    "\n",
    "# Boxplot for numeric variables\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=df[numeric_cols])\n",
    "plt.title(\"Boxplot for Numeric Variables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Scatterplot for total data vs applications (with log transform)\n",
    "app_cols = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',\n",
    "            'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']\n",
    "\n",
    "for col in app_cols:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Log-transform the data (avoid log(0) by adding a small constant)\n",
    "    log_x = np.log1p(df[col])  # log(1 + x) ensures no issues with zero values\n",
    "    log_y = np.log1p(df['total_data'])\n",
    "    \n",
    "    # Scatterplot with log-transformed data\n",
    "    sns.scatterplot(x=log_x, y=log_y)\n",
    "    plt.title(f\"Log-Transformed Total Data vs {col}\")\n",
    "    plt.xlabel(f\"Log {col}\")\n",
    "    plt.ylabel(\"Log Total Data\")\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Apply log transformation\n",
    "df['log_social_media_dl'] = np.log1p(df['Social Media DL (Bytes)'])\n",
    "df['log_total_data'] = np.log1p(df['total_data'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=df['log_social_media_dl'], y=df['log_total_data'], alpha=0.5)\n",
    "plt.title(\"Scatter Plot (Log-Transformed): Total Data vs Social Media DL\")\n",
    "plt.xlabel(\"Log(Social Media DL Bytes)\")\n",
    "plt.ylabel(\"Log(Total Data)\")\n",
    "plt.savefig(\"log_scatter_social_media_dl.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr_matrix = df[app_cols].corr()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(corr_matrix)\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.savefig(\"correlation_heatmap.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction (PCA)\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df[app_cols])\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Variance explained by each component\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot PCA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], s=10, alpha=0.6)\n",
    "plt.title(\"PCA Results\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating engagement metrics per user\n",
    "\n",
    "engagement_metrics = df.groupby('MSISDN/Number').agg({\n",
    "    'Dur. (ms)': 'sum',  \n",
    "    'Total DL (Bytes)': 'sum',   \n",
    "    'Total UL (Bytes)': 'sum',   \n",
    "    'Bearer Id': 'count'      \n",
    "}).rename(columns={\n",
    "    'Dur. (ms)': 'Total_Duration',\n",
    "    'Total DL (Bytes)': 'Total_DL',\n",
    "    'Total UL (Bytes)': 'Total_UL',\n",
    "    'Bearer Id': 'Session_Frequency'\n",
    "})\n",
    "\n",
    "# Total traffic (download + upload)\n",
    "engagement_metrics['Total_Traffic'] = engagement_metrics['Total_DL'] + engagement_metrics['Total_UL']\n",
    "\n",
    "# Top 10 customers per engagement metric\n",
    "top_10_duration = engagement_metrics.nlargest(10, 'Total_Duration')\n",
    "top_10_traffic = engagement_metrics.nlargest(10, 'Total_Traffic')\n",
    "top_10_frequency = engagement_metrics.nlargest(10, 'Session_Frequency')\n",
    "\n",
    "print(\"Top 10 customers by duration:\")\n",
    "print(top_10_duration)\n",
    "\n",
    "print(\"Top 10 customers by traffic:\")\n",
    "print(top_10_traffic)\n",
    "\n",
    "print(\"Top 10 customers by session frequency:\")\n",
    "print(top_10_frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Metrics and Perform K-Means Clustering\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Normalize the metrics\n",
    "scaler = MinMaxScaler()\n",
    "engagement_metrics_normalized = scaler.fit_transform(engagement_metrics[['Total_Duration', 'Total_Traffic', 'Session_Frequency']])\n",
    "\n",
    "# Run k-means (k=3)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "engagement_metrics['Cluster'] = kmeans.fit_predict(engagement_metrics_normalized)\n",
    "\n",
    "print(\"Cluster Assignments:\")\n",
    "print(engagement_metrics['Cluster'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Min, Max, Average, and Total Metrics for Each Cluster\n",
    "\n",
    "\n",
    "cluster_metrics = engagement_metrics.groupby('Cluster').agg({\n",
    "    'Total_Duration': ['min', 'max', 'mean', 'sum'],\n",
    "    'Total_Traffic': ['min', 'max', 'mean', 'sum'],\n",
    "    'Session_Frequency': ['min', 'max', 'mean', 'sum']\n",
    "})\n",
    "\n",
    "print(\"Cluster Metrics:\")\n",
    "print(cluster_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate user total traffic per application\n",
    "app_cols = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',\n",
    "            'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']\n",
    "\n",
    "application_metrics = []  # Store metrics for all applications\n",
    "\n",
    "for app_col in app_cols:\n",
    "    # Calculate total traffic for each user (MSISDN/Number) for this application\n",
    "    app_data = df.groupby('MSISDN/Number').agg({\n",
    "        app_col: 'sum',  # Aggregate the download traffic for this app\n",
    "        'Total UL (Bytes)': 'sum'  # Aggregate the upload traffic\n",
    "    }).rename(columns={\n",
    "        app_col: 'Total_DL',  # Rename download column\n",
    "        'Total UL (Bytes)': 'Total_UL'  # Rename upload column\n",
    "    })\n",
    "    \n",
    "    # Compute total traffic (download + upload)\n",
    "    app_data['Total_Traffic'] = app_data['Total_DL'] + app_data['Total_UL']\n",
    "    \n",
    "    # Add application name as a column\n",
    "    app_data['Application'] = app_col.replace(' DL (Bytes)', '')  # Clean app name\n",
    "    \n",
    "    # Append to application_metrics list\n",
    "    application_metrics.append(app_data)\n",
    "\n",
    "# Concatenate all application metrics into a single DataFrame\n",
    "application_metrics = pd.concat(application_metrics)\n",
    "\n",
    "# Top 10 users per application\n",
    "top_users_per_app = application_metrics.groupby('Application').apply(\n",
    "    lambda x: x.nlargest(10, 'Total_Traffic')\n",
    ")\n",
    "\n",
    "print(\"Top 10 users per application:\")\n",
    "print(top_users_per_app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the top 3 applications by total traffic\n",
    "top_apps = application_metrics.groupby('Application')['Total_Traffic'].sum().nlargest(3).index\n",
    "\n",
    "# Filter data for top 3 applications\n",
    "top_app_data = application_metrics[application_metrics['Application'].isin(top_apps)]\n",
    "top_app_data.to_csv(\"top_applications.csv\", index=False)\n",
    "\n",
    "# Plot traffic distribution\n",
    "\n",
    "sns.barplot(x='Application', y='Total_Traffic', data=top_app_data, estimator='sum', ci=None)\n",
    "plt.title(\"Top 3 Most Used Applications by Total Traffic\")\n",
    "plt.xlabel(\"Application\")\n",
    "plt.ylabel(\"Total Traffic (Bytes)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Elbow method\n",
    "sse = []\n",
    "k_range = range(1, 10)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(engagement_metrics_normalized)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "# Save results to a DataFrame\n",
    "elbow_data = pd.DataFrame({\"Number of Clusters (k)\": list(k_range), \"SSE\": sse})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "elbow_data.to_csv(\"elbow_method.csv\", index=False)\n",
    "print(\"Elbow method data saved to elbow_method.csv\")\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_range, sse, marker='o')\n",
    "plt.title(\"Elbow Method for Optimal K\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Sum of Squared Errors (SSE)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming 'clustered_data' contains your features\n",
    "features = clustered_data[['engagement_score', 'experience_score']]\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Calculate the Elbow Method values\n",
    "elbow_data = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(features_scaled)\n",
    "    elbow_data.append({'k': k, 'inertia': kmeans.inertia_})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "elbow_df = pd.DataFrame(elbow_data)\n",
    "elbow_df.to_csv('elbow_method.csv', index=False)\n",
    "print(\"Elbow method data saved to 'elbow_method.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data\n",
    "\n",
    "# Task 3.1 - Aggregation per customer\n",
    "def aggregate_metrics(df):\n",
    "    # Fill missing values\n",
    "    df['TCP DL Retrans. Vol (Bytes)'] = df['TCP DL Retrans. Vol (Bytes)'].fillna(df['TCP DL Retrans. Vol (Bytes)'].mean())\n",
    "    df['TCP UL Retrans. Vol (Bytes)'] = df['TCP UL Retrans. Vol (Bytes)'].fillna(df['TCP UL Retrans. Vol (Bytes)'].mean())\n",
    "    df['Avg RTT DL (ms)'] = df['Avg RTT DL (ms)'].fillna(df['Avg RTT DL (ms)'].mean())\n",
    "    df['Avg RTT UL (ms)'] = df['Avg RTT UL (ms)'].fillna(df['Avg RTT UL (ms)'].mean())\n",
    "    df['Avg Bearer TP DL (kbps)'] = df['Avg Bearer TP DL (kbps)'].fillna(df['Avg Bearer TP DL (kbps)'].mean())\n",
    "    df['Avg Bearer TP UL (kbps)'] = df['Avg Bearer TP UL (kbps)'].fillna(df['Avg Bearer TP UL (kbps)'].mean())\n",
    "\n",
    "    # Aggregate metrics\n",
    "    aggregated = df.groupby('MSISDN/Number').agg({\n",
    "        'TCP DL Retrans. Vol (Bytes)': 'mean',\n",
    "        'TCP UL Retrans. Vol (Bytes)': 'mean',\n",
    "        'Avg RTT DL (ms)': 'mean',\n",
    "        'Avg RTT UL (ms)': 'mean',\n",
    "        'Avg Bearer TP DL (kbps)': 'mean',\n",
    "        'Avg Bearer TP UL (kbps)': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    aggregated['Average TCP Retransmission'] = (\n",
    "        aggregated['TCP DL Retrans. Vol (Bytes)'] + aggregated['TCP UL Retrans. Vol (Bytes)']) / 2\n",
    "    aggregated['Average RTT'] = (\n",
    "        aggregated['Avg RTT DL (ms)'] + aggregated['Avg RTT UL (ms)']) / 2\n",
    "    aggregated['Average Throughput'] = (\n",
    "        aggregated['Avg Bearer TP DL (kbps)'] + aggregated['Avg Bearer TP UL (kbps)']) / 2\n",
    "\n",
    "    return aggregated\n",
    "\n",
    "aggregated_data = aggregate_metrics(df)\n",
    "\n",
    "# Task 3.2 - Top, bottom, and most frequent values\n",
    "def compute_top_bottom_most(df, column):\n",
    "    top_10 = df[column].nlargest(10)\n",
    "    bottom_10 = df[column].nsmallest(10)\n",
    "    most_frequent = df[column].value_counts().head(10)\n",
    "    return top_10, bottom_10, most_frequent\n",
    "\n",
    "tcp_top, tcp_bottom, tcp_frequent = compute_top_bottom_most(df, 'TCP DL Retrans. Vol (Bytes)')\n",
    "rtt_top, rtt_bottom, rtt_frequent = compute_top_bottom_most(df, 'Avg RTT DL (ms)')\n",
    "throughput_top, throughput_bottom, throughput_frequent = compute_top_bottom_most(df, 'Avg Bearer TP DL (kbps)')\n",
    "\n",
    "# Task 3.3 - Distribution & interpretation\n",
    "def plot_distribution(df, column, group_by, title, top_n=10):\n",
    "    \"\"\"\n",
    "    Plots the distribution of a specified column grouped by a category,\n",
    "    showing only the top N categories based on mean values.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - column: The column whose distribution is to be plotted.\n",
    "    - group_by: The column to group the data by.\n",
    "    - title: The title of the plot.\n",
    "    - top_n: The number of top categories to include in the plot (default is 10).\n",
    "    \"\"\"\n",
    "    # Group data by the specified column and compute the mean\n",
    "    grouped = df.groupby(group_by)[column].mean().dropna()\n",
    "\n",
    "    # Check if there is data to plot\n",
    "    if grouped.empty:\n",
    "        print(f\"No data to plot for {column} grouped by {group_by}\")\n",
    "        return\n",
    "\n",
    "    # Select the top N categories\n",
    "    top_categories = grouped.nlargest(top_n)\n",
    "\n",
    "    # Plot the data\n",
    "    top_categories.sort_values().plot(kind='bar', figsize=(12, 6), title=title)\n",
    "    plt.xlabel(group_by)\n",
    "    plt.ylabel(column)\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_distribution(df, 'Avg Bearer TP DL (kbps)', 'Handset Manufacturer', \n",
    "                  'Average Throughput per Manufacturer (Top 10)', top_n=10)\n",
    "plt.savefig(\"Average Troughput per Manufacurere .png\")\n",
    "\n",
    "plot_distribution(df, 'TCP DL Retrans. Vol (Bytes)', 'Handset Manufacturer', 'Average TCP Retransmission per Manufacturer (Top 10)', top_n=10)\n",
    "plt.savefig(\"Average TCP Retransmission per Manufacurere .png\")\n",
    "\n",
    "# Task 3.4 - K-means clustering\n",
    "def perform_clustering(df):\n",
    "    # Select relevant features\n",
    "    features = df[['Average TCP Retransmission', 'Average RTT', 'Average Throughput']]\n",
    "\n",
    "    # Normalize data\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    # K-means clustering\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "    clusters = kmeans.fit_predict(features_scaled)\n",
    "    df['Cluster'] = clusters\n",
    "\n",
    "    return df, kmeans\n",
    "\n",
    "clustered_data, kmeans_model = perform_clustering(aggregated_data)\n",
    "\n",
    "# Interpret clusters\n",
    "for cluster_id in range(3):\n",
    "    print(f\"Cluster {cluster_id}:\\n\")\n",
    "    cluster_data = clustered_data[clustered_data['Cluster'] == cluster_id]\n",
    "    print(cluster_data.describe())\n",
    "    print(\"\\n---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_engaged_center = kmeans_model.cluster_centers_[2]  # Example: replace 2 with the correct index\n",
    "worst_experience_center = kmeans_model.cluster_centers_[1]  # Example: replace 1 with the correct index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def calculate_scores(data, target_center):\n",
    "    return [euclidean(row, target_center) for row in data]\n",
    "\n",
    "# Calculate scores\n",
    "user_features = clustered_data[['Average TCP Retransmission', 'Average RTT', 'Average Throughput']].values\n",
    "clustered_data['engagement_score'] = calculate_scores(user_features, less_engaged_center)\n",
    "clustered_data['experience_score'] = calculate_scores(user_features, worst_experience_center)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_data['satisfaction_score'] = (\n",
    "    clustered_data['engagement_score'] + clustered_data['experience_score']\n",
    ") / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clustered_data[['engagement_score', 'experience_score']]\n",
    "y = clustered_data['satisfaction_score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R2 Score:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "clustered_data['satisfaction_cluster'] = kmeans.fit_predict(\n",
    "    clustered_data[['engagement_score', 'experience_score']]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=clustered_data,\n",
    "    x='engagement_score',\n",
    "    y='experience_score',\n",
    "    hue='satisfaction_cluster',\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title(\"K-Means Clustering on Engagement and Experience Scores\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_summary = clustered_data.groupby('satisfaction_cluster')[\n",
    "    ['satisfaction_score', 'experience_score']\n",
    "].mean()\n",
    "print(cluster_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the satisfaction score as the average of engagement and experience scores\n",
    "aggregated_data['Satisfaction Score'] = (aggregated_data['engagement_score'] + aggregated_data['experience_score']) / 2\n",
    "\n",
    "# Sort the customers by Satisfaction Score in descending order\n",
    "sorted_customers = aggregated_data.sort_values('Satisfaction Score', ascending=False)\n",
    "\n",
    "# Get the top 10 satisfied customers\n",
    "top_10_satisfied = sorted_customers.head(10)\n",
    "\n",
    "# Define final_results (this can contain all customers or only the top 10 depending on your needs)\n",
    "final_results = sorted_customers  # Or use final_results = top_10_satisfied if you only need top 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch DB credentials from environment variables\n",
    "db_user = os.getenv(\"DB_USER\")\n",
    "db_password = os.getenv(\"DB_PASSWORD\")\n",
    "db_host = os.getenv(\"DB_HOST\")\n",
    "db_port = os.getenv(\"DB_PORT\")\n",
    "db_name = os.getenv(\"DB_NAME\")\n",
    "\n",
    "# Create the connection string\n",
    "connection_string = f\"postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\"\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Export the DataFrame to PostgreSQL\n",
    "try:\n",
    "    # Save the entire final_results table\n",
    "    final_results.to_sql('customer_satisfaction', engine, if_exists='replace', index=False)\n",
    "    \n",
    "    # Save only the top 10 satisfied customers\n",
    "    top_10_satisfied.to_sql('top_satisfied_customers', engine, if_exists='replace', index=False)\n",
    "    \n",
    "    print(\"Data exported successfully to PostgreSQL!\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "final_results.to_csv(\"final_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "query = \"SELECT * FROM customer_satisfaction;\"\n",
    "sf = connect_to_db(query)\n",
    "if sf is not None:\n",
    "    print(\"successfuly connected\")\n",
    "else:\n",
    "    print(\"failed to connect\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
